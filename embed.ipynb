{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ai2/tokens/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 169.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight shape: torch.Size([152064, 3584])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"simplescaling/s1.1-7B\")\n",
    "\n",
    "embed_weights: torch.Tensor = model.model.embed_tokens.weight\n",
    "unembed_weights: torch.Tensor = model.lm_head.weight\n",
    "\n",
    "print(\"embed_tokens.weight shape:\", embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.5497e-02, -4.1463e-03,  1.4529e-02,  ...,  1.0730e-02,\n",
       "          4.1811e-02, -1.7753e-02],\n",
       "        [ 1.5380e-02,  1.5618e-02,  1.7398e-02,  ..., -1.9151e-02,\n",
       "          1.6571e-02,  7.5589e-03],\n",
       "        [-8.4275e-03, -4.7465e-03,  4.5070e-03,  ..., -1.0826e-03,\n",
       "         -2.7765e-02,  7.6024e-03],\n",
       "        ...,\n",
       "        [-1.1755e-37,  1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37,  1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37],\n",
       "        [ 1.1755e-37, -1.1755e-37, -1.1755e-37,  ...,  1.1755e-37,\n",
       "          1.1755e-37, -1.1755e-37]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea (doesn't work):\n",
    "    # model.embed_tokens.weight // lm_head.weight\n",
    "        # [152 064, 3 584] tensor\n",
    "        # we learn a [XXXXXX, 3 584] tensor which should be the same\n",
    "        # minimize L2 loss\n",
    "\n",
    "    # Then, we swap the embedding layer and re-run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The *only* way to learn this is by replacing the tokenizer and embedding layers, then continually re-training the embedding layers with the *entire* model\n",
    "\n",
    "# This is because the sequence lengths will change between tokenizers, so the only standard signal would be the text itself (not the next-token distribution, or the hidden representations)\n",
    "\n",
    "# You could re-run SFT (~1B-10B toks?) or mid-training (50B-300B toks) with the swapped model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
